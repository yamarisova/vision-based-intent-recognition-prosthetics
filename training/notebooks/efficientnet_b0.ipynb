{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOlfgE6kqinRp7+/xAtuoPK"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["!pip install --upgrade tensorflow mediapipe opencv-python scikit-learn ultralytics"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"23cGxaUcKu3g","executionInfo":{"status":"ok","timestamp":1756786996050,"user_tz":420,"elapsed":139726,"user":{"displayName":"Iana Marisova","userId":"07790758133999729730"}},"outputId":"5d194f7f-b6bc-4578-bb1d-be8f70d0d366"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: tensorflow in /usr/local/lib/python3.12/dist-packages (2.19.0)\n","Collecting tensorflow\n","  Downloading tensorflow-2.20.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.5 kB)\n","Collecting mediapipe\n","  Downloading mediapipe-0.10.21-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (9.7 kB)\n","Requirement already satisfied: opencv-python in /usr/local/lib/python3.12/dist-packages (4.12.0.88)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n","Collecting scikit-learn\n","  Downloading scikit_learn-1.7.1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (11 kB)\n","Collecting ultralytics\n","  Downloading ultralytics-8.3.191-py3-none-any.whl.metadata (37 kB)\n","Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.4.0)\n","Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.6.3)\n","Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (25.2.10)\n","Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.6.0)\n","Requirement already satisfied: google_pasta>=0.1.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.2.0)\n","Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (18.1.1)\n","Requirement already satisfied: opt_einsum>=2.3.2 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.4.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from tensorflow) (25.0)\n","Requirement already satisfied: protobuf>=5.28.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (5.29.5)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.32.4)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from tensorflow) (75.2.0)\n","Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.17.0)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.1.0)\n","Requirement already satisfied: typing_extensions>=3.6.6 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (4.15.0)\n","Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.17.3)\n","Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.74.0)\n","Collecting tensorboard~=2.20.0 (from tensorflow)\n","  Downloading tensorboard-2.20.0-py3-none-any.whl.metadata (1.8 kB)\n","Requirement already satisfied: keras>=3.10.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.10.0)\n","Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.0.2)\n","Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.14.0)\n","Requirement already satisfied: ml_dtypes<1.0.0,>=0.5.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.5.3)\n","Requirement already satisfied: attrs>=19.1.0 in /usr/local/lib/python3.12/dist-packages (from mediapipe) (25.3.0)\n","Requirement already satisfied: jax in /usr/local/lib/python3.12/dist-packages (from mediapipe) (0.5.3)\n","Requirement already satisfied: jaxlib in /usr/local/lib/python3.12/dist-packages (from mediapipe) (0.5.3)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from mediapipe) (3.10.0)\n","Collecting numpy>=1.26.0 (from tensorflow)\n","  Downloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: opencv-contrib-python in /usr/local/lib/python3.12/dist-packages (from mediapipe) (4.12.0.88)\n","INFO: pip is looking at multiple versions of mediapipe to determine which version is compatible with other requirements. This could take a while.\n","Collecting mediapipe\n","  Downloading mediapipe-0.10.20-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (9.7 kB)\n","  Downloading mediapipe-0.10.18-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.7 kB)\n","  Downloading mediapipe-0.10.15-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.7 kB)\n","  Downloading mediapipe-0.10.14-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.7 kB)\n","  Downloading mediapipe-0.10.13-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.7 kB)\n","Collecting tensorflow\n","  Downloading tensorflow-2.19.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n","Requirement already satisfied: tensorboard~=2.19.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.19.0)\n","Collecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 (from tensorflow)\n","  Downloading protobuf-4.25.8-cp37-abi3-manylinux2014_x86_64.whl.metadata (541 bytes)\n","Collecting sounddevice>=0.4.4 (from mediapipe)\n","  Downloading sounddevice-0.5.2-py3-none-any.whl.metadata (1.6 kB)\n","Requirement already satisfied: sentencepiece in /usr/local/lib/python3.12/dist-packages (from mediapipe) (0.2.1)\n","INFO: pip is looking at multiple versions of opencv-python to determine which version is compatible with other requirements. This could take a while.\n","Collecting opencv-python\n","  Downloading opencv_python-4.12.0.88-cp37-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (19 kB)\n","  Downloading opencv_python-4.11.0.86-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n","Requirement already satisfied: scipy>=1.8.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.16.1)\n","Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.1)\n","Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n","Requirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (11.3.0)\n","Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (6.0.2)\n","Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (2.8.0+cu126)\n","Requirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (0.23.0+cu126)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from ultralytics) (5.9.5)\n","Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.12/dist-packages (from ultralytics) (9.0.0)\n","Requirement already satisfied: polars in /usr/local/lib/python3.12/dist-packages (from ultralytics) (1.25.2)\n","Collecting ultralytics-thop>=2.0.0 (from ultralytics)\n","  Downloading ultralytics_thop-2.0.16-py3-none-any.whl.metadata (14 kB)\n","Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n","Requirement already satisfied: rich in /usr/local/lib/python3.12/dist-packages (from keras>=3.10.0->tensorflow) (13.9.4)\n","Requirement already satisfied: namex in /usr/local/lib/python3.12/dist-packages (from keras>=3.10.0->tensorflow) (0.1.0)\n","Requirement already satisfied: optree in /usr/local/lib/python3.12/dist-packages (from keras>=3.10.0->tensorflow) (0.17.0)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->mediapipe) (1.3.3)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->mediapipe) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->mediapipe) (4.59.1)\n","Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->mediapipe) (1.4.9)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->mediapipe) (3.2.3)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib->mediapipe) (2.9.0.post0)\n","Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.3)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.5.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.8.3)\n","Requirement already satisfied: CFFI>=1.0 in /usr/local/lib/python3.12/dist-packages (from sounddevice>=0.4.4->mediapipe) (1.17.1)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (3.8.2)\n","Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (0.7.2)\n","Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (3.1.3)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (3.19.1)\n","Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (1.13.3)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (3.5)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (3.1.6)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (2025.3.0)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.6.80)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (9.10.2.21)\n","Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.6.4.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (11.3.0.4)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (10.3.7.77)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (11.7.1.2)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.5.4.2)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (0.7.1)\n","Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (2.27.3)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.6.77)\n","Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.6.85)\n","Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (1.11.1.6)\n","Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (3.4.0)\n","INFO: pip is looking at multiple versions of opencv-contrib-python to determine which version is compatible with other requirements. This could take a while.\n","Collecting opencv-contrib-python (from mediapipe)\n","  Downloading opencv_contrib_python-4.11.0.86-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n","Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from CFFI>=1.0->sounddevice>=0.4.4->mediapipe) (2.22)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.8.0->ultralytics) (1.3.0)\n","Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.12/dist-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow) (3.0.2)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.10.0->tensorflow) (4.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.10.0->tensorflow) (2.19.2)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.10.0->tensorflow) (0.1.2)\n","Downloading tensorflow-2.19.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (645.0 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m645.0/645.0 MB\u001b[0m \u001b[31m835.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading mediapipe-0.10.21-cp312-cp312-manylinux_2_28_x86_64.whl (35.6 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m35.6/35.6 MB\u001b[0m \u001b[31m23.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading protobuf-4.25.8-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.9/294.9 kB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading opencv_python-4.11.0.86-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (63.0 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.0/63.0 MB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading scikit_learn-1.7.1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (9.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.5/9.5 MB\u001b[0m \u001b[31m112.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading ultralytics-8.3.191-py3-none-any.whl (1.1 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m48.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.0 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.0/18.0 MB\u001b[0m \u001b[31m94.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading sounddevice-0.5.2-py3-none-any.whl (32 kB)\n","Downloading ultralytics_thop-2.0.16-py3-none-any.whl (28 kB)\n","Downloading opencv_contrib_python-4.11.0.86-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (69.1 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.1/69.1 MB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: protobuf, numpy, sounddevice, opencv-python, opencv-contrib-python, scikit-learn, ultralytics-thop, tensorflow, ultralytics, mediapipe\n","  Attempting uninstall: protobuf\n","    Found existing installation: protobuf 5.29.5\n","    Uninstalling protobuf-5.29.5:\n","      Successfully uninstalled protobuf-5.29.5\n","  Attempting uninstall: numpy\n","    Found existing installation: numpy 2.0.2\n","    Uninstalling numpy-2.0.2:\n","      Successfully uninstalled numpy-2.0.2\n","  Attempting uninstall: opencv-python\n","    Found existing installation: opencv-python 4.12.0.88\n","    Uninstalling opencv-python-4.12.0.88:\n","      Successfully uninstalled opencv-python-4.12.0.88\n","  Attempting uninstall: opencv-contrib-python\n","    Found existing installation: opencv-contrib-python 4.12.0.88\n","    Uninstalling opencv-contrib-python-4.12.0.88:\n","      Successfully uninstalled opencv-contrib-python-4.12.0.88\n","  Attempting uninstall: scikit-learn\n","    Found existing installation: scikit-learn 1.6.1\n","    Uninstalling scikit-learn-1.6.1:\n","      Successfully uninstalled scikit-learn-1.6.1\n","  Attempting uninstall: tensorflow\n","    Found existing installation: tensorflow 2.19.0\n","    Uninstalling tensorflow-2.19.0:\n","      Successfully uninstalled tensorflow-2.19.0\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\n","grpcio-status 1.71.2 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 4.25.8 which is incompatible.\n","opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n","tensorflow-decision-forests 1.12.0 requires tensorflow==2.19.0, but you have tensorflow 2.19.1 which is incompatible.\n","ydf 0.13.0 requires protobuf<7.0.0,>=5.29.1, but you have protobuf 4.25.8 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed mediapipe-0.10.21 numpy-1.26.4 opencv-contrib-python-4.11.0.86 opencv-python-4.11.0.86 protobuf-4.25.8 scikit-learn-1.7.1 sounddevice-0.5.2 tensorflow-2.19.1 ultralytics-8.3.191 ultralytics-thop-2.0.16\n"]},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["google","numpy"]},"id":"c2e2e73d056a49419cc8c83169713ad3"}},"metadata":{}}]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"55JmhHLnKv8P","executionInfo":{"status":"ok","timestamp":1756787003931,"user_tz":420,"elapsed":593,"user":{"displayName":"Iana Marisova","userId":"07790758133999729730"}},"outputId":"636c335b-93da-4c00-db32-d73ea42d5f41"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1qOIJZ6VKA1sZgp1wY-iewWGxDweJ7TM_"},"id":"pl3L2iToKfcg","executionInfo":{"status":"ok","timestamp":1756792046843,"user_tz":420,"elapsed":149697,"user":{"displayName":"Iana Marisova","userId":"07790758133999729730"}},"outputId":"d7300afa-2c54-45f5-aeac-2617b7eda54d"},"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}],"source":["# ===================== CONFIG & IMPORTS =====================\n","import os, math, random, cv2, numpy as np, tensorflow as tf\n","from typing import List, Tuple\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import classification_report, confusion_matrix, f1_score\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import pandas as pd\n","\n","from tensorflow.keras import layers, regularizers, Model\n","from tensorflow.keras.optimizers.schedules import CosineDecay\n","from tensorflow.keras.optimizers import AdamW\n","from tensorflow.keras.utils import Sequence\n","from tensorflow.keras.applications import mobilenet, mobilenet_v2, resnet_v2, efficientnet\n","\n","import mediapipe as mp\n","mp_hands = mp.solutions.hands\n","\n","# ===== reproducibility\n","SEED = 42\n","random.seed(SEED); np.random.seed(SEED); tf.random.set_seed(SEED)\n","\n","# ===== WHAT TO TRAIN: \"mobilenet\", \"mobilenet_v2\", \"resnet50v2\", \"efficientnet_b0/b1/b2/b3\"\n","MODEL_NAME     = \"efficientnet_b0\"     # <-- змінюй тут\n","UNFREEZE_LAST  = 120                   # стадія 2: скільки останніх шарів розморозити\n","LABEL_SMOOTH   = 0.01\n","USE_FOCAL_LOSS = False                  # True=focal, False=cross-entropy\n","\n","# ===== Paths & Data\n","DATA_DIR   = \"/content/drive/MyDrive/MARISOVA_AI_HAND_PROJECT/Gestures_Dataset\"\n","VAL_SPLIT  = 0.2\n","BATCH_SIZE = 32\n","\n","# ===== ROI / Aug\n","MARGIN     = 1.8                       # ширше навколо руки\n","AUG        = True\n","\n","# ===== IMG_SIZE by backbone\n","if MODEL_NAME == \"mobilenet\":\n","    IMG_SIZE = (224, 224)\n","elif MODEL_NAME == \"mobilenet_v2\":\n","    IMG_SIZE = (224, 224)\n","elif MODEL_NAME == \"resnet50v2\":\n","    IMG_SIZE = (256, 256)\n","elif MODEL_NAME.startswith(\"efficientnet_b\"):\n","    size_map = {\n","        \"efficientnet_b0\": (224, 224),\n","        \"efficientnet_b1\": (240, 240),\n","        \"efficientnet_b2\": (260, 260),\n","        \"efficientnet_b3\": (300, 300),\n","    }\n","    IMG_SIZE = size_map.get(MODEL_NAME, (224, 224))\n","else:\n","    IMG_SIZE = (224, 224)\n","\n","# ===================== DATA SCAN =====================\n","def list_images_with_labels(root: str) -> Tuple[List[str], List[int], List[str]]:\n","    class_names = sorted([d for d in os.listdir(root) if os.path.isdir(os.path.join(root, d))])\n","    paths, labels = [], []\n","    for ci, cname in enumerate(class_names):\n","        cdir = os.path.join(root, cname)\n","        for fn in sorted(os.listdir(cdir)):\n","            if fn.lower().endswith((\".jpg\",\".jpeg\",\".png\",\".bmp\",\".webp\")):\n","                p = os.path.join(cdir, fn)\n","                if cv2.haveImageReader(p):\n","                    paths.append(p); labels.append(ci)\n","    return paths, labels, class_names\n","\n","paths_all, labels_all, CLASS_NAMES = list_images_with_labels(DATA_DIR)\n","print(f\"Класи: {CLASS_NAMES} | прикладів: {len(paths_all)}\")\n","\n","X_tr, X_val, y_tr, y_val = train_test_split(\n","    paths_all, labels_all, test_size=VAL_SPLIT, stratify=labels_all, random_state=SEED\n",")\n","\n","# ===================== ROI HELPERS (MediaPipe) =====================\n","def largest_hand_bbox_from_landmarks(img: np.ndarray, lms_list):\n","    h, w = img.shape[:2]\n","    best = None\n","    for lm in lms_list:\n","        xs = [int(p.x*w) for p in lm.landmark]\n","        ys = [int(p.y*h) for p in lm.landmark]\n","        x1, y1, x2, y2 = max(min(xs),0), max(min(ys),0), min(max(xs),w-1), min(max(ys),h-1)\n","        area = max(1,(x2-x1)*(y2-y1))\n","        if best is None or area > best[-1]:\n","            best = (x1, y1, x2, y2, area)\n","    if best is None:\n","        cx, cy = w//2, h//2; sz = min(w,h)//3\n","        return cx-sz, cy-sz, cx+sz, cy+sz\n","    x1,y1,x2,y2,_ = best\n","    cx, cy = (x1+x2)/2, (y1+y2)/2\n","    bw, bh = (x2-x1)*MARGIN, (y2-y1)*MARGIN\n","    x1n, y1n = int(max(0, cx-bw/2)), int(max(0, cy-bh/2))\n","    x2n, y2n = int(min(w-1, cx+bw/2)), int(min(h-1, cy+bh/2))\n","    return x1n, y1n, x2n, y2n\n","\n","def crop_hand_roi(img: np.ndarray, hands_ctx) -> np.ndarray:\n","    h, w = img.shape[:2]\n","    res = hands_ctx.process(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n","    if res.multi_hand_landmarks:\n","        x1,y1,x2,y2 = largest_hand_bbox_from_landmarks(img, res.multi_hand_landmarks)\n","    else:\n","        cx, cy = w//2, h//2; sz = min(w,h)//3\n","        x1,y1,x2,y2 = cx-sz, cy-sz, cx+sz, cy+sz\n","    roi = img[max(0,y1):min(h,y2), max(0,x1):min(w,x2)]\n","    if roi.size == 0: roi = img\n","    rh, rw = roi.shape[:2]\n","    side = max(rh, rw)\n","    pad_top = (side-rh)//2; pad_bottom = side-rh-pad_top\n","    pad_left = (side-rw)//2; pad_right = side-rw-pad_left\n","    roi = cv2.copyMakeBorder(roi, pad_top, pad_bottom, pad_left, pad_right,\n","                             borderType=cv2.BORDER_CONSTANT, value=(0,0,0))\n","    roi = cv2.resize(roi, IMG_SIZE, interpolation=cv2.INTER_AREA)\n","    return roi\n","\n","# ----- augmentations (blur + cutout + базові геометричні)\n","def random_cutout(img, max_frac=0.22):\n","    h, w = img.shape[:2]\n","    if np.random.rand() < 0.5:\n","        cw = int(np.random.uniform(0.05, max_frac)*w)\n","        ch = int(np.random.uniform(0.05, max_frac)*h)\n","        x1 = np.random.randint(0, w-cw+1); y1 = np.random.randint(0, h-ch+1)\n","        img[y1:y1+ch, x1:x1+cw] = 0\n","    return img\n","\n","def augment_rgb(img: np.ndarray) -> np.ndarray:\n","    h, w = img.shape[:2]\n","    if np.random.rand() < 0.5: img = cv2.flip(img, 1)\n","    if np.random.rand() < 0.5:\n","        ang = np.random.uniform(-20, 20)\n","        M = cv2.getRotationMatrix2D((w//2, h//2), ang, 1.0)\n","        img = cv2.warpAffine(img, M, (w, h), borderMode=cv2.BORDER_REFLECT101)\n","    if np.random.rand() < 0.4:\n","        tx = int(np.random.uniform(-0.10*w, 0.10*w))\n","        ty = int(np.random.uniform(-0.10*h, 0.10*h))\n","        M = np.float32([[1,0,tx],[0,1,ty]])\n","        img = cv2.warpAffine(img, M, (w, h), borderMode=cv2.BORDER_REFLECT101)\n","    if np.random.rand() < 0.4:\n","        sc = np.random.uniform(0.9, 1.1)\n","        M = cv2.getRotationMatrix2D((w//2, h//2), 0, sc)\n","        img = cv2.warpAffine(img, M, (w, h), borderMode=cv2.BORDER_REFLECT101)\n","    if np.random.rand() < 0.5:\n","        alpha = np.random.uniform(0.85, 1.20); beta = np.random.randint(-20, 20)\n","        img = cv2.convertScaleAbs(img, alpha=alpha, beta=beta)\n","    if np.random.rand() < 0.3:\n","        k = np.random.choice([3,5])\n","        img = cv2.GaussianBlur(img, (k,k), 0)\n","    img = random_cutout(img)\n","    return img\n","\n","# ===================== UNIVERSAL BACKBONE BUILDER =====================\n","def build_backbone(model_name: str, img_size, num_classes: int):\n","    \"\"\"\n","    Повертає (model, base, preprocess_fn) з «замороженим» бекбоном.\n","    Підтримує: mobilenet, mobilenet_v2, resnet50v2, efficientnet_b0/b1/b2/b3\n","    \"\"\"\n","    name = model_name.lower()\n","\n","    if name == \"mobilenet\":\n","        base = mobilenet.MobileNet(weights=\"imagenet\", include_top=False,\n","                                   input_shape=(*img_size, 3))\n","        preprocess_fn = mobilenet.preprocess_input\n","\n","    elif name == \"mobilenet_v2\":\n","        base = mobilenet_v2.MobileNetV2(weights=\"imagenet\", include_top=False,\n","                                        input_shape=(*img_size, 3))\n","        preprocess_fn = mobilenet_v2.preprocess_input\n","\n","    elif name == \"resnet50v2\":\n","        base = resnet_v2.ResNet50V2(weights=\"imagenet\", include_top=False,\n","                                    input_shape=(*img_size, 3))\n","        preprocess_fn = resnet_v2.preprocess_input\n","\n","    elif name in {\"efficientnet_b0\", \"efficientnet_b1\", \"efficientnet_b2\", \"efficientnet_b3\"}:\n","        eff_map = {\n","            \"efficientnet_b0\": efficientnet.EfficientNetB0,\n","            \"efficientnet_b1\": efficientnet.EfficientNetB1,\n","            \"efficientnet_b2\": efficientnet.EfficientNetB2,\n","            \"efficientnet_b3\": efficientnet.EfficientNetB3,\n","        }\n","        base = eff_map[name](weights=\"imagenet\", include_top=False, input_shape=(*img_size, 3))\n","        preprocess_fn = efficientnet.preprocess_input\n","\n","    else:\n","        raise ValueError(f\"Unknown MODEL_NAME: {model_name}\")\n","\n","    base.trainable = False\n","\n","    inp = layers.Input(shape=(*img_size, 3))\n","    x = base(inp, training=False)\n","\n","    # GAP + GMP → concat\n","    x_avg = layers.GlobalAveragePooling2D()(x)\n","    x_max = layers.GlobalMaxPooling2D()(x)\n","    x = layers.Concatenate()([x_avg, x_max])\n","\n","    dropout_rate = 0.4 if name.startswith(\"efficientnet_b\") else 0.3\n","    x = layers.Dense(256, kernel_regularizer=regularizers.l2(1e-5))(x)\n","    x = layers.BatchNormalization()(x)\n","    x = layers.Activation(\"relu\")(x)\n","    x = layers.Dropout(dropout_rate)(x)\n","\n","    out = layers.Dense(num_classes, activation=\"softmax\")(x)\n","    model = Model(inp, out)\n","    return model, base, preprocess_fn\n","\n","# ===================== SEQUENCE (ROI + preprocess_fn + MixUp) =====================\n","def mixup(X, y, alpha=0.2):\n","    if alpha is None or alpha <= 0 or len(X) < 2:\n","        return X, y\n","    lam = np.random.beta(alpha, alpha)\n","    idx = np.random.permutation(len(X))\n","    Xm = lam*X + (1-lam)*X[idx]\n","    ym = lam*y + (1-lam)*y[idx]\n","    return Xm, ym\n","\n","class ROICropSequence(Sequence):\n","    def __init__(self, paths, labels, batch_size, shuffle, augment,\n","                 num_classes, preprocess_fn):\n","        self.paths  = list(paths)\n","        self.labels = np.array(labels, dtype=np.int32)\n","        self.batch_size = batch_size\n","        self.shuffle = shuffle\n","        self.augment = augment\n","        self.num_classes = num_classes\n","        self.preprocess_fn = preprocess_fn\n","        self.idxs = np.arange(len(self.paths))\n","        self.hands = mp_hands.Hands(static_image_mode=True, max_num_hands=1,\n","                                    min_detection_confidence=0.5)\n","        self.on_epoch_end()\n","\n","    def __len__(self): return math.ceil(len(self.paths)/self.batch_size)\n","    def on_epoch_end(self):\n","        if self.shuffle: np.random.shuffle(self.idxs)\n","\n","    def _process_one(self, j):\n","        p = self.paths[j]\n","        img = cv2.imread(p)\n","        if img is None: return None\n","        roi = crop_hand_roi(img, self.hands)\n","        if self.augment: roi = augment_rgb(roi)\n","        roi = cv2.cvtColor(roi, cv2.COLOR_BGR2RGB).astype(np.float32)\n","        roi = self.preprocess_fn(roi)\n","        return roi, self.labels[j]\n","\n","    def __getitem__(self, i):\n","        ids = self.idxs[i*self.batch_size:(i+1)*self.batch_size]\n","        X, y = [], []\n","        for j in ids:\n","            item = self._process_one(j)\n","            if item is None: continue\n","            xi, yi = item; X.append(xi); y.append(yi)\n","        if len(X) == 0:\n","            for j in range(len(self.paths)):\n","                item = self._process_one(j)\n","                if item is not None:\n","                    xi, yi = item; X=[xi]; y=[yi]; break\n","            if len(X)==0: raise ValueError(\"ROICropSequence: немає валідних зображень.\")\n","        X = np.stack(X, axis=0)\n","        y = tf.keras.utils.to_categorical(np.array(y), num_classes=self.num_classes)\n","        if self.augment:\n","            X, y = mixup(X, y, alpha=0.2)\n","        return X, y\n","\n","    def close(self): self.hands.close()\n","\n","# ===================== FOCAL LOSS (опційно) =====================\n","class CategoricalFocalLoss(tf.keras.losses.Loss):\n","    def __init__(self, gamma=2.0, alpha=None, label_smoothing=0.0):\n","        super().__init__()\n","        self.gamma = gamma\n","        self.alpha = alpha\n","        self.ce = tf.keras.losses.CategoricalCrossentropy(label_smoothing=label_smoothing)\n","\n","    def call(self, y_true, y_pred):\n","        y_pred = tf.clip_by_value(y_pred, 1e-7, 1.0-1e-7)\n","        ce = tf.keras.losses.categorical_crossentropy(y_true, y_pred)\n","        pt = tf.reduce_sum(y_true * y_pred, axis=-1)\n","        focal = tf.pow(1. - pt, self.gamma) * ce\n","        if self.alpha is not None:\n","            alpha_w = tf.reduce_sum(y_true * tf.constant(self.alpha, dtype=y_true.dtype), axis=-1)\n","            focal *= alpha_w\n","        return focal\n","\n","# ===================== BUILD MODEL & DATA LOADERS =====================\n","num_classes = len(CLASS_NAMES)\n","model, base, PREPROC = build_backbone(MODEL_NAME, IMG_SIZE, num_classes)\n","\n","train_seq = ROICropSequence(X_tr, y_tr, batch_size=BATCH_SIZE, shuffle=True,\n","                            augment=True, num_classes=num_classes, preprocess_fn=PREPROC)\n","val_seq   = ROICropSequence(X_val, y_val, batch_size=BATCH_SIZE, shuffle=False,\n","                            augment=False, num_classes=num_classes, preprocess_fn=PREPROC)\n","\n","# Клас-ваги (опційно). Залиши 1.0,1.0,1.0,1.0 якщо не хочеш ваг.\n","hard = {\"Intent_Grasp\", \"Intent_Release\"}\n","CLASS_WEIGHT = {i: (1.0 if CLASS_NAMES[i] in hard else 1.0) for i in range(num_classes)}\n","print(\"class_weight:\", CLASS_WEIGHT)\n","\n","# ===================== MACRO-F1 CALLBACK =====================\n","class MacroF1Callback(tf.keras.callbacks.Callback):\n","    def __init__(self, val_seq, name='val_macro_f1'):\n","        super().__init__()\n","        self.val_seq = val_seq\n","        self.name = name\n","    def on_epoch_end(self, epoch, logs=None):\n","        y_true, y_pred = [], []\n","        for b in range(len(self.val_seq)):\n","            Xb, yb = self.val_seq[b]\n","            prob = self.model.predict(Xb, verbose=0)\n","            y_true.extend(np.argmax(yb, axis=1))\n","            y_pred.extend(np.argmax(prob, axis=1))\n","        m = f1_score(y_true, y_pred, average='macro')\n","        logs = logs or {}\n","        logs[self.name] = m\n","        print(f\"\\n[Callback] {self.name}: {m:.4f}\")\n","\n","macro_cb = MacroF1Callback(val_seq)\n","\n","# ===================== LOSS & OPTIMIZER (Stage 1) =====================\n","if USE_FOCAL_LOSS:\n","    alpha_vec = [1.0 if cls not in hard else 1.5 for cls in CLASS_NAMES]\n","    loss_obj = CategoricalFocalLoss(gamma=2.0, alpha=alpha_vec, label_smoothing=LABEL_SMOOTH)\n","else:\n","    loss_obj = tf.keras.losses.CategoricalCrossentropy(label_smoothing=LABEL_SMOOTH)\n","\n","steps_per_epoch = max(1, len(train_seq))\n","lr1 = CosineDecay(initial_learning_rate=3e-4, decay_steps=steps_per_epoch*8)\n","opt1 = AdamW(learning_rate=lr1, weight_decay=1e-4)\n","model.compile(optimizer=opt1, loss=loss_obj, metrics=['accuracy'])\n","\n","ckpt_dir = \"/content/drive/MyDrive/MARISOVA_AI_HAND_PROJECT/models\"\n","os.makedirs(ckpt_dir, exist_ok=True)\n","ckpt_path = f\"{ckpt_dir}/{MODEL_NAME}_roi_best.keras\"\n","\n","cbs_stage1 = [\n","    macro_cb,\n","    tf.keras.callbacks.ModelCheckpoint(ckpt_path, save_best_only=True,\n","                                       monitor='val_macro_f1', mode='max'),\n","    tf.keras.callbacks.EarlyStopping(monitor='val_macro_f1', mode='max',\n","                                     patience=8, restore_best_weights=True),\n","    tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5,\n","                                         patience=3, min_lr=1e-6, verbose=1)\n","]\n","\n","print(\"\\n=== Stage 1 ===\")\n","hist1 = model.fit(train_seq, validation_data=val_seq,\n","                  epochs=8, class_weight=CLASS_WEIGHT, callbacks=cbs_stage1, verbose=2)\n","\n","# ===================== TRAIN: Stage 2 (fine-tune tail) =====================\n","# для EfficientNet за замовчуванням UNFREEZE_LAST=80 (зверху)\n","for layer in base.layers[-UNFREEZE_LAST:]:\n","    if not isinstance(layer, layers.BatchNormalization):\n","        layer.trainable = True\n","\n","lr2 = CosineDecay(initial_learning_rate=3e-5, decay_steps=steps_per_epoch*12)\n","opt2 = AdamW(learning_rate=lr2, weight_decay=1e-4)\n","model.compile(optimizer=opt2, loss=loss_obj, metrics=['accuracy'])\n","\n","macro_cb2 = MacroF1Callback(val_seq)\n","cbs_stage2 = [\n","    macro_cb2,\n","    tf.keras.callbacks.ModelCheckpoint(ckpt_path, save_best_only=True,\n","                                       monitor='val_macro_f1', mode='max'),\n","    tf.keras.callbacks.EarlyStopping(monitor='val_macro_f1', mode='max',\n","                                     patience=8, restore_best_weights=True),\n","    tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5,\n","                                         patience=3, min_lr=1e-6, verbose=1)\n","]\n","\n","print(\"\\n=== Stage 2 ===\")\n","hist2 = model.fit(train_seq, validation_data=val_seq,\n","                  epochs=12, class_weight=CLASS_WEIGHT, callbacks=cbs_stage2, verbose=2)\n","\n","print(\"Best model saved to:\", ckpt_path)\n","\n","# ===================== EVALUATION on VAL =====================\n","y_true_list, y_pred_list, y_prob_list = [], [], []\n","for b in range(len(val_seq)):\n","    Xb, yb = val_seq[b]\n","    prob = model.predict(Xb, verbose=0)\n","    y_prob_list.append(prob)\n","    y_true_list.extend(np.argmax(yb, axis=1))\n","    y_pred_list.extend(np.argmax(prob, axis=1))\n","\n","y_true = np.array(y_true_list)\n","y_pred = np.array(y_pred_list)\n","y_prob = np.concatenate(y_prob_list, axis=0)\n","\n","print(classification_report(y_true, y_pred, target_names=CLASS_NAMES, digits=3))\n","\n","cm = confusion_matrix(y_true, y_pred, labels=list(range(len(CLASS_NAMES))))\n","plt.figure(figsize=(6,5))\n","sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n","            xticklabels=CLASS_NAMES, yticklabels=CLASS_NAMES)\n","plt.xlabel(\"Predicted\"); plt.ylabel(\"True\"); plt.title(\"Confusion Matrix (ROI)\")\n","plt.tight_layout(); plt.show()\n","\n","# ===================== TRAINING CURVES (Stage1 + Stage2) =====================\n","plt.rcParams.update({\n","    \"font.size\": 14,\n","    \"axes.labelweight\": \"bold\",\n","    \"axes.titlesize\": 16,\n","    \"axes.titleweight\": \"bold\",\n","    \"legend.fontsize\": 12,\n","    \"xtick.labelsize\": 12,\n","    \"ytick.labelsize\": 12,\n","    \"grid.alpha\": 0.3\n","})\n","\n","epochs_all = range(1, len(hist1.history['accuracy']) + len(hist2.history['accuracy']) + 1)\n","train_acc = hist1.history['accuracy'] + hist2.history['accuracy']\n","val_acc   = hist1.history['val_accuracy'] + hist2.history['val_accuracy']\n","train_loss = hist1.history['loss'] + hist2.history['loss']\n","val_loss   = hist1.history['val_loss'] + hist2.history['val_loss']\n","\n","stage_boundary = len(hist1.history['accuracy'])\n","lr_curve = [3e-4]*stage_boundary + [3e-5]*len(hist2.history['loss'])\n","\n","fig, axs = plt.subplots(2, 2, figsize=(12,8))\n","\n","axs[0,0].plot(epochs_all, train_acc, '-o', label='Train Accuracy')\n","axs[0,0].plot(epochs_all, val_acc, '-o', label='Validation Accuracy')\n","axs[0,0].axvline(stage_boundary, color='gray', linestyle='--', label='Stage boundary')\n","axs[0,0].set_title(\"Accuracy (Stage1 + Stage2)\")\n","axs[0,0].set_xlabel(\"Epoch\"); axs[0,0].set_ylabel(\"Accuracy\")\n","axs[0,0].legend(); axs[0,0].grid(True)\n","\n","axs[0,1].plot(epochs_all, train_loss, '-o', label='Train Loss')\n","axs[0,1].plot(epochs_all, val_loss, '-o', label='Validation Loss')\n","axs[0,1].axvline(stage_boundary, color='gray', linestyle='--')\n","axs[0,1].set_title(\"Loss (Stage1 + Stage2)\")\n","axs[0,1].set_xlabel(\"Epoch\"); axs[0,1].set_ylabel(\"Loss\")\n","axs[0,1].legend(); axs[0,1].grid(True)\n","\n","axs[1,0].plot(epochs_all, lr_curve, '-o')\n","axs[1,0].axvline(stage_boundary, color='gray', linestyle='--')\n","axs[1,0].set_title(\"Learning Rate (nominal)\")\n","axs[1,0].set_xlabel(\"Epoch\"); axs[1,0].set_ylabel(\"LR\")\n","axs[1,0].grid(True)\n","\n","axs[1,1].axis(\"off\")\n","plt.tight_layout(); plt.show()\n","\n","# ======= РОЗШИРЕНИЙ VAL-РЕПОРТ + ПЛОТИ =======\n","report_str = classification_report(y_true, y_pred, target_names=CLASS_NAMES, digits=3)\n","cm_raw = confusion_matrix(y_true, y_pred, labels=list(range(len(CLASS_NAMES))))\n","cm_norm = cm_raw.astype(float) / (cm_raw.sum(axis=1, keepdims=True) + 1e-12)\n","\n","plt.figure(figsize=(6,5))\n","sns.heatmap(cm_raw, annot=True, fmt='d', cmap='Blues',\n","            xticklabels=CLASS_NAMES, yticklabels=CLASS_NAMES)\n","plt.title(\"Validation Confusion Matrix (raw)\"); plt.xlabel(\"Predicted\"); plt.ylabel(\"True\")\n","plt.tight_layout(); plt.show()\n","\n","plt.figure(figsize=(6,5))\n","sns.heatmap(cm_norm, annot=True, fmt='.2f', cmap='Greens',\n","            xticklabels=CLASS_NAMES, yticklabels=CLASS_NAMES, vmin=0, vmax=1)\n","plt.title(\"Validation Confusion Matrix (normalized by true)\"); plt.xlabel(\"Predicted\"); plt.ylabel(\"True\")\n","plt.tight_layout(); plt.show()\n","\n","report_dict = classification_report(y_true, y_pred, target_names=CLASS_NAMES, output_dict=True)\n","df_scores = pd.DataFrame({m: [report_dict[c][m] for c in CLASS_NAMES]\n","                          for m in ['precision','recall','f1-score']},\n","                         index=CLASS_NAMES)\n","\n","plt.figure(figsize=(10,4))\n","plt.subplot(1,3,1); sns.barplot(x=df_scores.index, y=df_scores['precision']); plt.title('Precision'); plt.ylim(0,1); plt.xticks(rotation=20)\n","plt.subplot(1,3,2); sns.barplot(x=df_scores.index, y=df_scores['recall']);    plt.title('Recall');    plt.ylim(0,1); plt.xticks(rotation=20)\n","plt.subplot(1,3,3); sns.barplot(x=df_scores.index, y=df_scores['f1-score']);  plt.title('F1-score');  plt.ylim(0,1); plt.xticks(rotation=20)\n","plt.suptitle(\"Validation per-class metrics\"); plt.tight_layout(); plt.show()\n","\n","overall_acc = (y_true == y_pred).mean()\n","macro_f1 = df_scores['f1-score'].mean()\n","print(f\"[VAL] Overall accuracy: {overall_acc:.3f}  |  Macro-F1: {macro_f1:.3f}\")\n","\n","# ====== ПРИВ’Ю ПРАВИЛЬНИХ/НЕПРАВИЛЬНИХ ПРИКЛАДІВ (із уже препроцесених батчів) ======\n","def _approx_deprocess(img):\n","    \"\"\"\n","    Робимо картинку видимою: для [-1,1] -> [0,1]; інакше min-max нормалізація.\n","    Працює для мобі/еффішент/реснет без точного інверту.\n","    \"\"\"\n","    mn, mx = img.min(), img.max()\n","    if mx <= 1.0 and mn >= -1.0:\n","        img = (img + 1.0) * 0.5\n","    else:\n","        img = (img - mn) / (mx - mn + 1e-8)\n","    return np.clip(img, 0, 1)\n","\n","# збираємо всі X,y,prob з val_seq (без повторного MediaPipe)\n","X_all, y_all, prob_all = [], [], []\n","for b in range(len(val_seq)):\n","    Xb, yb = val_seq[b]                  # вже ROI+preprocess\n","    pb = model.predict(Xb, verbose=0)\n","    X_all.append(Xb); y_all.append(np.argmax(yb,1)); prob_all.append(pb)\n","X_all = np.concatenate(X_all, 0)\n","y_all = np.concatenate(y_all, 0)\n","prob_all = np.concatenate(prob_all, 0)\n","pred_all = np.argmax(prob_all, 1)\n","conf_all = prob_all.max(axis=1)\n","\n","idx_correct = np.where(pred_all == y_all)[0]\n","idx_wrong   = np.where(pred_all != y_all)[0]\n","idx_correct = idx_correct[np.argsort(-conf_all[idx_correct])]\n","idx_wrong   = idx_wrong[np.argsort(-conf_all[idx_wrong])]\n","\n","def show_grid(indices, title, k=16):\n","    m = min(k, len(indices))\n","    cols = 4; rows = int(np.ceil(m/cols))\n","    plt.figure(figsize=(12, 3*rows))\n","    for i in range(m):\n","        j = indices[i]\n","        plt.subplot(rows, cols, i+1)\n","        plt.imshow(_approx_deprocess(X_all[j]))\n","        plt.title(f\"T:{CLASS_NAMES[y_all[j]]}\\nP:{CLASS_NAMES[pred_all[j]]}\\nconf:{conf_all[j]:.2f}\")\n","        plt.axis('off')\n","    plt.suptitle(title); plt.tight_layout(); plt.show()\n","\n","show_grid(idx_correct, \"CORRECT predictions (top-confident)\", k=16)\n","show_grid(idx_wrong,   \"MISCLASSIFIED predictions (top-confident errors)\", k=16)\n","\n","# ====== ЗБЕРЕЖЕННЯ РЕПОРТУ ======\n","SAVE_DIR = \"/content/drive/MyDrive/MARISOVA_AI_HAND_PROJECT/reports\"\n","os.makedirs(SAVE_DIR, exist_ok=True)\n","with open(os.path.join(SAVE_DIR, f\"{MODEL_NAME}_val_report.txt\"), \"w\", encoding=\"utf-8\") as f:\n","    f.write(f\"=== VALIDATION REPORT (ROI CNN, {MODEL_NAME}) ===\\n\\n\")\n","    f.write(report_str + \"\\n\")\n","    f.write(f\"\\nOverall accuracy: {overall_acc:.3f}\\nMacro-F1: {macro_f1:.3f}\\n\")\n","\n","# закриваємо MediaPipe контексти\n","train_seq.close(); val_seq.close()\n"]}]}